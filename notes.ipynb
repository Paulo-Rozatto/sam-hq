{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas sobre HQ-SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset é composto por imagens de diferentes dimensões que são redimensionados para 1024x1024, ou para a resolução passada pela flag `--input_size` ao rodar o script de treino.\n",
    "\n",
    "As anotações do dataset são as máscaras de segmentação dos objetos. \n",
    "Não tem anotado especificamente os tokens de entrada (bounding box, pontos ou noise_mask).\n",
    "\n",
    "O que acontece é que a partir das máscaras ground truth, são geradas em tempo de treino tokens de entrada aleatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 407 - ln 4016 ##\n",
    "input_keys = ['box','point','noise_mask']\n",
    "labels_box = misc.masks_to_boxes(labels[:,0,:,:])\n",
    "try:\n",
    "    labels_points = misc.masks_sample_points(labels[:,0,:,:])\n",
    "except:\n",
    "    # less than 10 points\n",
    "    input_keys = ['box','noise_mask']\n",
    "    labels_256 = F.interpolate(labels, size=(256, 256), mode='bilinear')\n",
    "    labels_noisemask = misc.masks_noise(labels_256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após gerar os tokens, um tipo aleatório é escolhido para cada imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 417 - ln 434 ##\n",
    "batched_input = []\n",
    "for b_i in range(len(imgs)):\n",
    "    dict_input = dict()\n",
    "    input_image = torch.as_tensor(imgs[b_i].astype(dtype=np.uint8), device=sam.device).permute(2, 0, 1).contiguous()\n",
    "    dict_input['image'] = input_image \n",
    "    input_type = random.choice(input_keys)\n",
    "    if input_type == 'box':\n",
    "        dict_input['boxes'] = labels_box[b_i:b_i+1]\n",
    "    elif input_type == 'point':\n",
    "        point_coords = labels_points[b_i:b_i+1]\n",
    "        dict_input['point_coords'] = point_coords\n",
    "        dict_input['point_labels'] = torch.ones(point_coords.shape[1], device=point_coords.device)[None,:]\n",
    "    elif input_type == 'noise_mask':\n",
    "        dict_input['mask_inputs'] = labels_noisemask[b_i:b_i+1]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    dict_input['original_size'] = imgs[b_i].shape[:2]\n",
    "    batched_input.append(dict_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo menos durante o treino, a capacidade do SAM de gerar múltiplas mascáras de saída é desligada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 436 - ln 437 ##\n",
    "with torch.no_grad():\n",
    "    batched_output, interm_embeddings = sam(batched_input, multimask_output=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
