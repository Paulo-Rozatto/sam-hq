{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notas sobre HQ-SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset do SAM-HQ é composto por imagens de diferentes dimensões que são redimensionados para 1024x1024 usando interpolação bilinear. A flag `--input_size` permite mudar a resolução ao rodar o script de treino.\n",
    "\n",
    "> Uma nota é que o dataset do sam original tem um formato diferente, a segmentação está em formato COCO RLE. Mais sobre [isso aqui](https://github.com/facebookresearch/segment-anything/tree/main#dataset).\n",
    "\n",
    "Além de redimensionamento, há outros pré-processamentos: inversão aleatória de eixo horizontal e `LargeScaleJitter`.\n",
    "\n",
    "O `LargeScaleJitter` faz basicamente 3 coisas:\n",
    "1. Redimensiona aleatoriamente;\n",
    "2. Corta aleatoriamente;\n",
    "3. Preenche com cinza (padding);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 323 ##\n",
    "\n",
    "if not args.eval:\n",
    "    print(\"--- create training dataloader ---\")\n",
    "    train_im_gt_list = get_im_gt_name_dict(train_datasets, flag=\"train\")\n",
    "    train_dataloaders, train_datasets = create_dataloaders(train_im_gt_list,\n",
    "                                                    my_transforms = [\n",
    "                                                                RandomHFlip(),\n",
    "                                                                LargeScaleJitter()\n",
    "                                                                ],\n",
    "                                                    batch_size = args.batch_size_train,\n",
    "                                                    training = True)\n",
    "    print(len(train_dataloaders), \" train dataloaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## os padroes da classe declarados antes do trecho selecinado abaixo ##\n",
    "self = {\"desired_size\": 1024, \"aug_scale_min\": 0.1, \"aug_scale_max\": 2.0}\n",
    "\n",
    "##  dataloader.py ln 164 ##\n",
    "imidx, image, label, image_size =  sample['imidx'], sample['image'], sample['label'], sample['shape']\n",
    "\n",
    "#resize keep ratio\n",
    "\n",
    "# essa variável abaixo é declarada, mas não é usada\n",
    "# out_desired_size = (self.desired_size * image_size / max(image_size)).round().int()\n",
    "\n",
    "random_scale = torch.rand(1) * (self.aug_scale_max - self.aug_scale_min) + self.aug_scale_min\n",
    "scaled_size = (random_scale * self.desired_size).round()\n",
    "\n",
    "scale = torch.minimum(scaled_size / image_size[0], scaled_size / image_size[1])\n",
    "scaled_size = (image_size * scale).round().long()\n",
    "        \n",
    "scaled_image = torch.squeeze(F.interpolate(torch.unsqueeze(image,0),scaled_size.tolist(),mode='bilinear'),dim=0)\n",
    "scaled_label = torch.squeeze(F.interpolate(torch.unsqueeze(label,0),scaled_size.tolist(),mode='bilinear'),dim=0)\n",
    "        \n",
    "# random crop\n",
    "crop_size = (min(self.desired_size, scaled_size[0]), min(self.desired_size, scaled_size[1]))\n",
    "\n",
    "margin_h = max(scaled_size[0] - crop_size[0], 0).item()\n",
    "margin_w = max(scaled_size[1] - crop_size[1], 0).item()\n",
    "offset_h = np.random.randint(0, margin_h + 1)\n",
    "offset_w = np.random.randint(0, margin_w + 1)\n",
    "crop_y1, crop_y2 = offset_h, offset_h + crop_size[0].item()\n",
    "crop_x1, crop_x2 = offset_w, offset_w + crop_size[1].item()\n",
    "\n",
    "scaled_image = scaled_image[:,crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "scaled_label = scaled_label[:,crop_y1:crop_y2, crop_x1:crop_x2]\n",
    "\n",
    "# pad\n",
    "padding_h = max(self.desired_size - scaled_image.size(1), 0).item()\n",
    "padding_w = max(self.desired_size - scaled_image.size(2), 0).item()\n",
    "image = F.pad(scaled_image, [0,padding_w, 0,padding_h],value=128)\n",
    "label = F.pad(scaled_label, [0,padding_w, 0,padding_h],value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anotações do dataset são as máscaras de segmentação dos objetos. \n",
    "Não tem anotado especificamente os tokens de entrada (bounding box, pontos ou noise_mask).\n",
    "\n",
    "O que acontece é que a partir das máscaras ground truth, são geradas em tempo de treino tokens de entrada aleatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 407 - ln 4016 ##\n",
    "input_keys = ['box','point','noise_mask']\n",
    "labels_box = misc.masks_to_boxes(labels[:,0,:,:])\n",
    "try:\n",
    "    labels_points = misc.masks_sample_points(labels[:,0,:,:])\n",
    "except:\n",
    "    # less than 10 points\n",
    "    input_keys = ['box','noise_mask']\n",
    "    labels_256 = F.interpolate(labels, size=(256, 256), mode='bilinear')\n",
    "    labels_noisemask = misc.masks_noise(labels_256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após gerar os tokens, um tipo aleatório é escolhido para cada imagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 417 - ln 434 ##\n",
    "batched_input = []\n",
    "for b_i in range(len(imgs)):\n",
    "    dict_input = dict()\n",
    "    input_image = torch.as_tensor(imgs[b_i].astype(dtype=np.uint8), device=sam.device).permute(2, 0, 1).contiguous()\n",
    "    dict_input['image'] = input_image \n",
    "    input_type = random.choice(input_keys)\n",
    "    if input_type == 'box':\n",
    "        dict_input['boxes'] = labels_box[b_i:b_i+1]\n",
    "    elif input_type == 'point':\n",
    "        point_coords = labels_points[b_i:b_i+1]\n",
    "        dict_input['point_coords'] = point_coords\n",
    "        dict_input['point_labels'] = torch.ones(point_coords.shape[1], device=point_coords.device)[None,:]\n",
    "    elif input_type == 'noise_mask':\n",
    "        dict_input['mask_inputs'] = labels_noisemask[b_i:b_i+1]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    dict_input['original_size'] = imgs[b_i].shape[:2]\n",
    "    batched_input.append(dict_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pelo menos durante o treino, a capacidade do SAM de gerar múltiplas mascáras de saída é desligada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train.py ln 436 - ln 437 ##\n",
    "with torch.no_grad():\n",
    "    batched_output, interm_embeddings = sam(batched_input, multimask_output=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
